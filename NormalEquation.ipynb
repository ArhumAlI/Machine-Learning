{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Training and Test Data, and adding a column of ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "len = x_train.shape[1]*x_train.shape[2]\n",
    "x_train = x_train.reshape(-1,len)\n",
    "x_test = x_test.reshape(-1,len)\n",
    "x_train = np.insert(x_train, 0, values=1, axis=1)\n",
    "x_test = np.insert(x_test, 0, values=1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float64')\n",
    "x_test = x_test.astype('float64')\n",
    "# y_train = y_train.astype('float64')\n",
    "# y_test = y_test.astype('float64')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = x_train.shape[1]\n",
    "records = x_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X,W):\n",
    "    predicted = np.dot(W, X)\n",
    "    if predicted > 0: return 1\n",
    "    else: return -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyLabels(y_train,model):\n",
    "    y = np.copy(y_train)\n",
    "    for i in range(records):\n",
    "        if y[i] == model: \n",
    "            y[i] = 1.0\n",
    "        else: \n",
    "            y[i] = -1.0\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(y,res,model):\n",
    "    \n",
    "    for i in range(y_test.shape[0]):\n",
    "         if res[i] == 1:\n",
    "            res[i] = model     \n",
    "    s = accuracy_score(y, res, normalize=False)\n",
    "    print(\"score\")\n",
    "    print(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalEQ(model, res):  \n",
    "    W = np.random.randn(examples)\n",
    "    y = copyLabels(y_train,model)\n",
    "\n",
    "    W = np.linalg.pinv(np.transpose(x_train).dot(x_train)).dot(np.transpose(x_train)).dot(y)\n",
    "    \n",
    "    for i in range(x_test.shape[0]):\n",
    "        res.append(prediction(x_test[i],W))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0\n",
      "score\n",
      "864\n",
      "model 1\n",
      "score\n",
      "1131\n",
      "model 2\n",
      "score\n",
      "939\n",
      "model 3\n",
      "score\n",
      "980\n",
      "model 4\n",
      "score\n",
      "972\n",
      "model 5\n",
      "score\n",
      "859\n",
      "model 6\n",
      "score\n",
      "809\n",
      "model 7\n",
      "score\n",
      "910\n",
      "model 8\n",
      "score\n",
      "967\n",
      "model 9\n",
      "score\n",
      "1003\n",
      "9434\n",
      "Accuracy = 94.34\n"
     ]
    }
   ],
   "source": [
    "tn = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    res = []\n",
    "    print(\"model \"+ str(i))\n",
    "    \n",
    "    newPredicted = NormalEQ(i, res)\n",
    "    score = getAccuracy(y_test,newPredicted,i)\n",
    "    total += score    \n",
    "print(total)\n",
    "\n",
    "accuracy = total/10000\n",
    "print(\"Accuracy = \"+str(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
